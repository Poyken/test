# AI Flow Configuration

# LLM Provider Settings
llm:
  # Primary provider: gemini (free), ollama (local), or openrouter (openai-compatible)
  provider: ollama

  # OpenRouter settings (OpenAI Compatible)
  openrouter:
    base_url: https://openrouter.ai/api/v1
    model: meta-llama/llama-3.3-70b-instruct:free
    temperature: 0.1
    max_tokens: 8192

  # Gemini settings (Free tier: 15 RPM, 1M tokens/day) (Deprecated usage)
  gemini:
    model: gemini-pro-latest
    temperature: 0.1
    max_tokens: 8192

  # Ollama settings (for local GPU)
  ollama:
    base_url: http://localhost:11434
    model: qwen2.5-coder:7b
    temperature: 0.1
    max_tokens: 8192

# Agent-specific model overrides
# Agent-specific model overrides (Using global settings)
agents:
  pm_agent:
    # model: gemini-2.0-flash-exp
    temperature: 0.3

  architect_agent:
    # model: gemini-2.0-flash-exp
    temperature: 0.2

  task_agent:
    # model: gemini-2.0-flash-exp
    temperature: 0.1

  code_agent:
    # model: gemini-2.0-flash-exp
    temperature: 0.1

  review_agent:
    # model: gemini-2.0-flash-exp
    temperature: 0.2

  qa_agent:
    # model: gemini-2.0-flash-exp
    temperature: 0.1

# Tech Stack Constraints
tech_stack:
  backend:
    framework: nestjs
    language: typescript
    orm: prisma
    database: postgresql
    cache: redis
    queue: bullmq

  frontend:
    framework: nextjs
    language: typescript
    styling: tailwindcss
    state: zustand

# Workflow Settings
workflow:
  max_retries: 3
  retry_delay_seconds: 5
  checkpoint_after_phase: true
  auto_approve_trivial_changes: true

# Output Settings
output:
  base_dir: ./generated
  create_git_commits: true
  run_linting: true
  run_tests: true

# RAG Settings
rag:
  enabled: true
  embedding_model: models/embedding-001
  chunk_size: 1000
  chunk_overlap: 200
  top_k: 5
